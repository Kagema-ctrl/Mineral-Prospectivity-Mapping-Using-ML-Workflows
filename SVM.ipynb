{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, classification_report\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to your data\n",
    "ml_folder = r\"C:\\Users\\USER\\Desktop\\PROJECTS\\ESRI\\DATA\\RASTERS_ALIGNED\"\n",
    "\n",
    "# Output directories\n",
    "OUT_DIR = r\"C:\\Users\\USER\\Desktop\\PROJECTS\\ESRI\\DATA\\OUTPUTS\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Training data: 409 rows and 50 columns\n"
     ]
    }
   ],
   "source": [
    "# Load training points data\n",
    "arcpy.env.workspace = gdb\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "training_points_fc = r\"C:\\Users\\USER\\Desktop\\PROJECTS\\ESRI\\DATA\\MPM_LIRHANDA_CORRIDOR\\MPM_LIRHANDA_CORRIDOR.gdb\\TrainingPts\"\n",
    "# --- Read Training Points Data into Pandas DataFrame ---\n",
    "fields = [f.name for f in arcpy.ListFields(training_points_fc)]\n",
    "# Assume label field is named \"Label\" and coordinates are \"x\", \"y\"\n",
    "skip_fields = [\"ObjectID\", \"Shape\", \"Shape_Length\", \"Shape_Area\"]  # Adjust this based on your dataset\n",
    "\n",
    "# Extract features and labels\n",
    "rows = []\n",
    "with arcpy.da.SearchCursor(training_points_fc, fields) as cursor:\n",
    "    for row in cursor:\n",
    "        data = {fields[i]: row[i] for i in range(len(row)) if fields[i] not in skip_fields}\n",
    "        rows.append(data)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "print(f\"[Info] Training data: {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "y = df['Label'].values  # Label column assumed to be 'Label'\n",
    "X = df.drop(columns=['Label'])  # Drop the label column from the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Preprocessing Pipelines ---\n",
    "# Column names\n",
    "cont_cols = [col for col in X.columns if X[col].dtype in ['float64', 'int64']]  # Continuous columns\n",
    "lith_cols = [col for col in X.columns if 'lith_' in col]  # Lithology one-hot encoded columns\n",
    "\n",
    "# Preprocessing pipeline for the SVM\n",
    "svm_prep = ColumnTransformer([\n",
    "    (\"cont_pipe\", Pipeline([\n",
    "        (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scale\", StandardScaler())\n",
    "    ]), cont_cols),\n",
    "    (\"lith_impute\", SimpleImputer(strategy=\"most_frequent\"), lith_cols)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "# --- Combine All Features (Lithology, Geochemical, Continuous) ---\n",
    "feature_order = cont_cols + lith_cols  # All features we want to include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SVM Model Definition ---\n",
    "def build_svm(C=5.0):\n",
    "    \"\"\"Build SVM pipeline with RBF kernel, scale continuous features, and balanced classes.\"\"\"\n",
    "    return Pipeline([\n",
    "        (\"prep\", svm_prep),  # Apply preprocessing\n",
    "        (\"svc\", SVC(kernel=\"rbf\", C=C, gamma=\"scale\", class_weight=\"balanced\", probability=True, random_state=42))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fishnet created: C:\\Users\\USER\\Desktop\\PROJECTS\\ESRI\\DATA\\MPM_LIRHANDA_CORRIDOR\\MPM_LIRHANDA_CORRIDOR.gdb\\Fishnet_5km\n",
      "Spatial join complete: C:\\Users\\USER\\Desktop\\PROJECTS\\ESRI\\DATA\\MPM_LIRHANDA_CORRIDOR\\MPM_LIRHANDA_CORRIDOR.gdb\\TrainingPts_with_block_id\n",
      "DataFrame with block_id: 409 rows and 54 columns\n"
     ]
    }
   ],
   "source": [
    "import arcpy\n",
    "\n",
    "# Define paths for the fishnet and training points\n",
    "gdb = r\"C:\\Users\\USER\\Desktop\\PROJECTS\\ESRI\\DATA\\MPM_LIRHANDA_CORRIDOR\\MPM_LIRHANDA_CORRIDOR.gdb\"\n",
    "training_points_fc = r\"C:\\Users\\USER\\Desktop\\PROJECTS\\ESRI\\DATA\\MPM_LIRHANDA_CORRIDOR\\MPM_LIRHANDA_CORRIDOR.gdb\\TrainingPts\"\n",
    "fishnet_fc = os.path.join(gdb, \"Fishnet_5km\")  # Path to the fishnet\n",
    "fishnet_cell_size = 5000  # Define the cell size (e.g., 5000 meters)\n",
    "\n",
    "# Step 1: Create the fishnet over your AOI\n",
    "# Get the extent of the AOI (or use the extent of the training points)\n",
    "extent = arcpy.Describe(training_points_fc).extent\n",
    "origin = f\"{extent.XMin} {extent.YMin}\"\n",
    "y_axis = f\"{extent.XMin} {extent.YMin + 10}\"  # Small offset to define Y axis direction\n",
    "opposite_corner = f\"{extent.XMax} {extent.YMax}\"\n",
    "\n",
    "# Create fishnet\n",
    "if arcpy.Exists(fishnet_fc):\n",
    "    arcpy.management.Delete(fishnet_fc)  # Delete if it already exists\n",
    "\n",
    "arcpy.management.CreateFishnet(\n",
    "    out_feature_class=fishnet_fc,\n",
    "    origin_coord=origin,\n",
    "    y_axis_coord=y_axis,\n",
    "    cell_width=fishnet_cell_size,\n",
    "    cell_height=fishnet_cell_size,\n",
    "    number_rows=0,  # Automatically calculate the number of rows based on extent\n",
    "    number_columns=0,  # Automatically calculate the number of columns based on extent\n",
    "    corner_coord=opposite_corner,\n",
    "    labels=\"NO_LABELS\",\n",
    "    template=\"#\",\n",
    "    geometry_type=\"POLYGON\"\n",
    ")\n",
    "\n",
    "print(f\"Fishnet created: {fishnet_fc}\")\n",
    "\n",
    "# Step 2: Add a block_id field to the fishnet (if not already exists)\n",
    "if \"block_id\" not in [f.name for f in arcpy.ListFields(fishnet_fc)]:\n",
    "    arcpy.management.AddField(fishnet_fc, \"block_id\", \"LONG\")\n",
    "\n",
    "# Step 3: Populate the block_id field with the OID (Object ID) from the fishnet\n",
    "oid_field = arcpy.Describe(fishnet_fc).OIDFieldName\n",
    "arcpy.management.CalculateField(fishnet_fc, \"block_id\", f\"!{oid_field}!\", \"PYTHON3\")\n",
    "\n",
    "# Step 4: Spatially join the training points with the fishnet\n",
    "# This will assign a 'block_id' to each point in your training data\n",
    "output_fc = r\"C:\\Users\\USER\\Desktop\\PROJECTS\\ESRI\\DATA\\MPM_LIRHANDA_CORRIDOR\\MPM_LIRHANDA_CORRIDOR.gdb\\TrainingPts_with_block_id\"\n",
    "\n",
    "arcpy.analysis.SpatialJoin(\n",
    "    target_features=training_points_fc,\n",
    "    join_features=fishnet_fc,\n",
    "    out_feature_class=output_fc,\n",
    "    join_type=\"KEEP_COMMON\",  # Only keep points that intersect with fishnet\n",
    "    match_option=\"INTERSECT\"\n",
    ")\n",
    "\n",
    "print(f\"Spatial join complete: {output_fc}\")\n",
    "\n",
    "# Step 5: Add block_id to the DataFrame (assuming you are using pandas)\n",
    "# Read the resulting feature class into a pandas DataFrame\n",
    "fields = [f.name for f in arcpy.ListFields(output_fc)]\n",
    "rows = []\n",
    "with arcpy.da.SearchCursor(output_fc, fields) as cursor:\n",
    "    for row in cursor:\n",
    "        rows.append(row)\n",
    "\n",
    "# Create DataFrame from rows\n",
    "df_with_block_id = pd.DataFrame(rows, columns=fields)\n",
    "print(f\"DataFrame with block_id: {df_with_block_id.shape[0]} rows and {df_with_block_id.shape[1]} columns\")\n",
    "\n",
    "# Now you can use `df_with_block_id` for your machine learning tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: Index(['OBJECTID', 'Shape', 'Join_Count', 'TARGET_FID', 'Label', 'TxtLabel',\n",
      "       'Ag', 'As', 'Bi', 'ClayAIOH', 'Cu', 'Curvature_Gen10', 'Curv_Plan_10',\n",
      "       'Curv_Profile_10', 'dem10', 'dem10_1', 'distfaults2', 'FebyMn',\n",
      "       'Ferrous', 'ironoxide', 'KbyAl', 'KbyNa', 'KbyRb', 'KbyTi', 'OM', 'Pb',\n",
      "       'PH', 'Sb', 'Slope_10', 'Tl', 'UbyK', 'Zn', 'Lith_And_11',\n",
      "       'Lith_BIF_15', 'Lith_Bpl_17', 'Lith_Bsl_13', 'Lith_Bs_43', 'Lith_Cgl_5',\n",
      "       'Lith_Dac_12', 'Lith_Dd_28', 'Lith_Gc_21', 'Lith_Gl_20', 'Lith_Gnt_40',\n",
      "       'Lith_Gn_18', 'Lith_Grt_4', 'Lith_Kv_41', 'Lith_Mud_3', 'Lith_Ny_42',\n",
      "       'Lith_Phy_39', 'Lith_Rhy_10', 'Lith_Sy_23', 'Lith_Tuf_14', 'Lith_Tv_31',\n",
      "       'block_id'],\n",
      "      dtype='object')\n",
      "Number of rows: 409\n",
      "C= 1  OOF ROC-AUC=0.979 | OOF PR-AUC=0.893\n",
      "C= 2  OOF ROC-AUC=0.981 | OOF PR-AUC=0.893\n",
      "C= 5  OOF ROC-AUC=0.982 | OOF PR-AUC=0.909\n",
      "C=10  OOF ROC-AUC=0.975 | OOF PR-AUC=0.904\n",
      "Chosen C (by OOF PR-AUC): 5\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset that already contains 'block_id'\n",
    "training_points_with_block_id_fc = r\"C:\\Users\\USER\\Desktop\\PROJECTS\\ESRI\\DATA\\MPM_LIRHANDA_CORRIDOR\\MPM_LIRHANDA_CORRIDOR.gdb\\TrainingPts_with_block_id\"\n",
    "\n",
    "# Convert the feature class to a DataFrame\n",
    "fields = [f.name for f in arcpy.ListFields(training_points_with_block_id_fc)]\n",
    "rows = []\n",
    "with arcpy.da.SearchCursor(training_points_with_block_id_fc, fields) as cursor:\n",
    "    for row in cursor:\n",
    "        rows.append(row)\n",
    "\n",
    "df_with_block_id = pd.DataFrame(rows, columns=fields)\n",
    "\n",
    "# Check the columns to ensure 'block_id' is present\n",
    "print(f\"Columns: {df_with_block_id.columns}\")\n",
    "print(f\"Number of rows: {df_with_block_id.shape[0]}\")\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "y = df_with_block_id['Label'].values  # Assuming 'Label' is the column for the target variable\n",
    "X = df_with_block_id.drop(columns=['Label', 'block_id'])  # Drop 'Label' and 'block_id' from features\n",
    "\n",
    "# Define continuous and lithology columns (modify based on your dataset)\n",
    "cont_cols = [col for col in X.columns if X[col].dtype in ['float64', 'int64']]  # Continuous columns\n",
    "lith_cols = [col for col in X.columns if 'lith_' in col]  # Lithology one-hot encoded columns\n",
    "\n",
    "# Preprocessing pipeline for the SVM\n",
    "svm_prep = ColumnTransformer([\n",
    "    (\"cont_pipe\", Pipeline([\n",
    "        (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scale\", StandardScaler())\n",
    "    ]), cont_cols),\n",
    "    (\"lith_impute\", SimpleImputer(strategy=\"most_frequent\"), lith_cols)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "# SVM model pipeline\n",
    "def build_svm(C=5.0):\n",
    "    return Pipeline([\n",
    "        (\"prep\", svm_prep),  # Apply preprocessing\n",
    "        (\"svc\", SVC(kernel=\"rbf\", C=C, gamma=\"scale\", class_weight=\"balanced\", probability=True, random_state=42))\n",
    "    ])\n",
    "\n",
    "# --- Cross-validation: GroupKFold ---\n",
    "C_grid = [1, 2, 5, 10]  # Hyperparameter tuning grid for C\n",
    "best_C, best_pr, oof_best = None, -1.0, None\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# GroupKFold cross-validation\n",
    "for C in C_grid:\n",
    "    oof = np.zeros(len(y))  # Out-of-Fold predictions\n",
    "    \n",
    "    for tr, te in gkf.split(X, y, df_with_block_id['block_id']):  # Use 'block_id' for grouping\n",
    "        model = build_svm(C).fit(X.iloc[tr], y[tr])\n",
    "        oof[te] = model.predict_proba(X.iloc[te])[:, 1]  # Probability for class 1\n",
    "    \n",
    "    # Evaluate using PR-AUC and ROC-AUC\n",
    "    pr = average_precision_score(y, oof)\n",
    "    roc = roc_auc_score(y, oof)\n",
    "    \n",
    "    print(f\"C={C:>2}  OOF ROC-AUC={roc:.3f} | OOF PR-AUC={pr:.3f}\")\n",
    "    if pr > best_pr:\n",
    "        best_pr, best_C, oof_best = pr, C, oof\n",
    "\n",
    "print(f\"Chosen C (by OOF PR-AUC): {best_C}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM OOF threshold (F2-opt): 0.082  |  P=0.702  R=0.971\n",
      "OOF ROC-AUC: 0.982 | OOF PR-AUC: 0.909\n"
     ]
    }
   ],
   "source": [
    "# --- Threshold Selection: PR Curve (F2-optimal) ---\n",
    "prec, rec, thr = precision_recall_curve(y, oof_best)\n",
    "beta = 2  # F2 score focuses on recall\n",
    "f2 = (1 + beta**2) * (prec * rec) / ((beta**2) * prec + rec + 1e-9)\n",
    "k = f2.argmax()\n",
    "THRESH_SVM = float(thr[max(k-1, 0)])  # Optimal threshold from PR curve\n",
    "\n",
    "print(f\"SVM OOF threshold (F2-opt): {THRESH_SVM:.3f}  |  P={prec[k]:.3f}  R={rec[k]:.3f}\")\n",
    "print(f\"OOF ROC-AUC: {roc_auc_score(y, oof_best):.3f} | OOF PR-AUC: {average_precision_score(y, oof_best):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SVM: Spatial hold-out (25% blocks) ===\n",
      "ROC-AUC: 0.986 | PR-AUC: 0.951\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.976     0.932     0.953        88\n",
      "           1      0.769     0.909     0.833        22\n",
      "\n",
      "    accuracy                          0.927       110\n",
      "   macro avg      0.873     0.920     0.893       110\n",
      "weighted avg      0.935     0.927     0.929       110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure you're using the DataFrame that contains 'block_id'\n",
    "# If you used `TrainingPts_with_block_id`, use that DataFrame here.\n",
    "df = df_with_block_id  # Make sure this is the DataFrame with the 'block_id'\n",
    "\n",
    "# --- Spatial Hold-out Evaluation ---\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.25, random_state=456)\n",
    "\n",
    "# Use 'block_id' from the correct DataFrame (df_with_block_id)\n",
    "tr, te = next(gss.split(X, y, df['block_id']))  # Ensure you're using df with block_id\n",
    "\n",
    "svm_hold = build_svm(best_C).fit(X.iloc[tr], y[tr])\n",
    "proba_te = svm_hold.predict_proba(X.iloc[te])[:, 1]  # Test set probabilities\n",
    "\n",
    "print(\"\\n=== SVM: Spatial hold-out (25% blocks) ===\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y[te], proba_te):.3f} | PR-AUC: {average_precision_score(y[te], proba_te):.3f}\")\n",
    "pred_te = (proba_te >= THRESH_SVM).astype(int)\n",
    "print(classification_report(y[te], pred_te, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM model is ready for deployment. Variables available: `svm` and `THRESH_SVM`.\n"
     ]
    }
   ],
   "source": [
    "# --- Final Model Training on All Data ---\n",
    "svm = build_svm(best_C).fit(X, y)\n",
    "print(\"\\nSVM model is ready for deployment. Variables available: `svm` and `THRESH_SVM`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model expects 52 columns.\n",
      "[WARN] These expected columns have no raster on disk; will be filled with zeros:\n",
      "       OBJECTID, Shape, Join_Count, TARGET_FID, TxtLabel\n",
      "Processed rows 8192-8938 cols 8192-9031\n",
      "[INFO] Finished probability prediction.\n",
      "[OK] Saved probability raster: C:\\Users\\USER\\Desktop\\PROJECTS\\ESRI\\DATA\\OUTPUTS\\Prospectivity_SVM.tif\n",
      "[OK] Saved binary raster:      C:\\Users\\USER\\Desktop\\PROJECTS\\ESRI\\DATA\\OUTPUTS\\Targets_SVM_binary.tif\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import arcpy\n",
    "from arcpy.sa import *\n",
    "\n",
    "# --- ArcGIS setup ---\n",
    "arcpy.CheckOutExtension(\"Spatial\")\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "# ====== CONFIG / INPUTS ======\n",
    "try:\n",
    "    OUT_DIR\n",
    "except NameError:\n",
    "    OUT_DIR = r\"C:\\Users\\USER\\Desktop\\PROJECTS\\ESRI\\DATA\\OUTPUTS\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "ml_folder = r\"C:\\Users\\USER\\Desktop\\PROJECTS\\ESRI\\DATA\\RASTERS_ALIGNED\"\n",
    "OUT_PROB  = os.path.join(OUT_DIR, \"Prospectivity_SVM.tif\")\n",
    "OUT_BIN   = os.path.join(OUT_DIR, \"Targets_SVM_binary.tif\")\n",
    "\n",
    "# ====== HELPERS ======\n",
    "def norm_name(s: str) -> str:\n",
    "    stem = os.path.splitext(os.path.basename(s))[0]\n",
    "    return ''.join(ch for ch in stem.lower() if ch.isalnum() or ch == '_')\n",
    "\n",
    "def read_block(raster_path: str, lower_left_point: arcpy.Point, w: int, h: int) -> np.ndarray:\n",
    "    ras = arcpy.Raster(raster_path)\n",
    "    arr = arcpy.RasterToNumPyArray(ras, lower_left_point, w, h).astype(\"float32\")\n",
    "    nd = ras.noDataValue\n",
    "    if nd is not None:\n",
    "        arr[arr == nd] = np.nan\n",
    "    return arr\n",
    "\n",
    "def safe_save_tiff(ras, out_path, pixel_type=\"32_BIT_FLOAT\", nodata=None):\n",
    "    if arcpy.Exists(out_path):\n",
    "        arcpy.management.Delete(out_path)\n",
    "    try:\n",
    "        ras.save(out_path)\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Direct save failed for {os.path.basename(out_path)} -> {e}\")\n",
    "        scratch_gdb = arcpy.env.scratchGDB\n",
    "        tmp_name = arcpy.CreateUniqueName(\"tmp_save\", scratch_gdb)\n",
    "        ras.save(tmp_name)\n",
    "        if arcpy.Exists(out_path):\n",
    "            arcpy.management.Delete(out_path)\n",
    "        arcpy.management.CopyRaster(\n",
    "            in_raster=tmp_name,\n",
    "            out_rasterdataset=out_path,\n",
    "            pixel_type=pixel_type,\n",
    "            nodata_value=nodata,\n",
    "            format=\"TIFF\"\n",
    "        )\n",
    "        arcpy.management.Delete(tmp_name)\n",
    "\n",
    "# ====== MODEL-EXPECTED COLUMNS ======\n",
    "# Prefer the exact columns the pipeline was fit on.\n",
    "if hasattr(svm, \"feature_names_in_\"):\n",
    "    required_cols = list(svm.feature_names_in_)\n",
    "else:\n",
    "    required_cols = list(feature_order)  # fall back if needed\n",
    "\n",
    "print(f\"[INFO] Model expects {len(required_cols)} columns.\")\n",
    "\n",
    "# ====== BUILD LOOKUP FROM FILES ======\n",
    "tif_files = [f for f in os.listdir(ml_folder) if f.lower().endswith(\".tif\")]\n",
    "feat_lookup = {norm_name(f): os.path.join(ml_folder, f) for f in tif_files}\n",
    "\n",
    "# Split expected columns into those with rasters (“band features”) vs. missing (admin/others)\n",
    "band_features = [c for c in required_cols if norm_name(c) in feat_lookup]\n",
    "missing_feats = [c for c in required_cols if norm_name(c) not in feat_lookup]\n",
    "\n",
    "if missing_feats:\n",
    "    print(\"[WARN] These expected columns have no raster on disk; will be filled with zeros:\")\n",
    "    print(\"       \" + \", \".join(missing_feats))\n",
    "\n",
    "# ====== REFERENCE RASTER ======\n",
    "ref_raster_path = os.path.join(ml_folder, \"dem10.tif\")\n",
    "if not arcpy.Exists(ref_raster_path):\n",
    "    raise FileNotFoundError(f\"Reference raster not found: {ref_raster_path}\")\n",
    "\n",
    "ref_raster = arcpy.Raster(ref_raster_path)\n",
    "ncols = int(ref_raster.width)\n",
    "nrows = int(ref_raster.height)\n",
    "cell_w = ref_raster.meanCellWidth\n",
    "cell_h = ref_raster.meanCellHeight\n",
    "xmin  = ref_raster.extent.XMin\n",
    "ymin  = ref_raster.extent.YMin\n",
    "\n",
    "nodata_out = -9999.0\n",
    "out_prob = np.full((nrows, ncols), np.nan, dtype=\"float32\")\n",
    "\n",
    "# ====== BLOCKED PREDICTION ======\n",
    "BLOCK = 1024\n",
    "\n",
    "for r0 in range(0, nrows, BLOCK):\n",
    "    rh = min(BLOCK, nrows - r0)\n",
    "    for c0 in range(0, ncols, BLOCK):\n",
    "        cw = min(BLOCK, ncols - c0)\n",
    "        ll_win = arcpy.Point(xmin + c0 * cell_w, ymin + r0 * cell_h)\n",
    "\n",
    "        # Read only the band-backed features\n",
    "        band_list = []\n",
    "        for f in band_features:\n",
    "            f_path = feat_lookup[norm_name(f)]\n",
    "            band_list.append(read_block(f_path, ll_win, cw, rh))\n",
    "        if band_list:\n",
    "            stack = np.stack(band_list, axis=0)  # [n_band_features, rh, cw]\n",
    "            X_block = pd.DataFrame(\n",
    "                stack.reshape(len(band_features), -1).T,\n",
    "                columns=band_features\n",
    "            )\n",
    "        else:\n",
    "            # No band features? create empty frame with correct length\n",
    "            X_block = pd.DataFrame(index=np.arange(rh*cw))\n",
    "\n",
    "        # Fill missing columns (admin etc.) with zeros, then reorder to model expectation\n",
    "        for col in missing_feats:\n",
    "            X_block[col] = 0.0\n",
    "        # Make sure all required columns exist\n",
    "        missing_now = [c for c in required_cols if c not in X_block.columns]\n",
    "        for col in missing_now:\n",
    "            X_block[col] = 0.0\n",
    "        X_block = X_block[required_cols].fillna(0.0)\n",
    "\n",
    "        # Predict proba for class 1 and reshape to the tile\n",
    "        proba = svm.predict_proba(X_block)[:, 1].astype(\"float32\").reshape(rh, cw)\n",
    "\n",
    "        out_prob[r0:r0 + rh, c0:c0 + cw] = proba\n",
    "        print(f\"Processed rows {r0}-{r0+rh} cols {c0}-{c0+cw}\", end=\"\\r\")\n",
    "\n",
    "print(\"\\n[INFO] Finished probability prediction.\")\n",
    "\n",
    "# ====== THRESHOLD + SAVE ======\n",
    "threshold = 0.5\n",
    "out_bin = np.zeros_like(out_prob, dtype=\"uint8\")\n",
    "valid = ~np.isnan(out_prob)\n",
    "out_bin[valid & (out_prob >= threshold)] = 1\n",
    "\n",
    "ll_ref = arcpy.Point(xmin, ymin)\n",
    "prob_ras = arcpy.NumPyArrayToRaster(out_prob, ll_ref, cell_w, cell_h, value_to_nodata=nodata_out)\n",
    "bin_ras  = arcpy.NumPyArrayToRaster(out_bin,  ll_ref, cell_w, cell_h, value_to_nodata=None)\n",
    "\n",
    "safe_save_tiff(prob_ras, OUT_PROB, pixel_type=\"32_BIT_FLOAT\", nodata=nodata_out)\n",
    "safe_save_tiff(bin_ras,  OUT_BIN,  pixel_type=\"8_BIT_UNSIGNED\", nodata=None)\n",
    "\n",
    "print(f\"[OK] Saved probability raster: {OUT_PROB}\")\n",
    "print(f\"[OK] Saved binary raster:      {OUT_BIN}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All steps completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the probability raster output\n",
    "to_save = np.where(np.isfinite(out_prob), out_prob, nodata_out)\n",
    "rast_p = arcpy.NumPyArrayToRaster(to_save, ref_raster.extent.lowerLeft, ref_raster.meanCellWidth, \n",
    "                                   ref_raster.meanCellHeight, nodata_out)\n",
    "rast_p.save(OUT_PROB)\n",
    "\n",
    "# Save binary targets based on threshold\n",
    "bin_arr = np.zeros_like(to_save, dtype=\"uint8\")\n",
    "valid_prob = np.isfinite(out_prob)\n",
    "bin_arr[valid_prob] = (out_prob[valid_prob] >= THRESH_SVM).astype(\"uint8\")\n",
    "nodata_bin = 255  # For NoData values\n",
    "bin_arr[~valid_prob] = nodata_bin\n",
    "rast_b = arcpy.NumPyArrayToRaster(bin_arr, ref_raster.extent.lowerLeft, ref_raster.meanCellWidth, \n",
    "                                  ref_raster.meanCellHeight, nodata_bin)\n",
    "rast_b.save(OUT_BIN)\n",
    "\n",
    "\n",
    "# Build pyramids for better visualization\n",
    "arcpy.management.CalculateStatistics(OUT_PROB)\n",
    "arcpy.management.BuildPyramids(OUT_PROB)\n",
    "arcpy.management.CalculateStatistics(OUT_BIN)\n",
    "arcpy.management.BuildPyramids(OUT_BIN)\n",
    "\n",
    "print(\"✅ All steps completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
